<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>发卡</title>
  
  <subtitle>2019</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-07-23T16:13:42.996Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>miiiss</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>爬虫常用库方法总结</title>
    <link href="http://example.com/2022/07/23/%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://example.com/2022/07/23/%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2022-07-23T10:48:13.000Z</published>
    <updated>2022-07-23T16:13:42.996Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>网页图片批量获取：</p><h2 id="图片下载："><a href="#图片下载：" class="headerlink" title="图片下载："></a>图片下载：</h2><p><em><strong>urllib库</strong></em>  </p><blockquote><p>urllib.request-打开和读取url<br>urllib.error-用来抛出异常<br>urllib.parse-解析url<br>urllib.robotpaser-解析robots.txt<br><img src="https://www.runoob.com/wp-content/uploads/2021/04/ulrib-py3.svg" alt="urllib">  </p></blockquote><p>urllib.request 定义了一些打开 URL 的函数和类，包含授权验证、重定向、浏览器 cookies等。</p><p>urllib.request 可以模拟浏览器的一个请求发起过程。</p><p>我们可以使用 urllib.request 的 urlopen 方法来打开一个 URL，语法格式如下</p><p>python<br>urllib.request.urlopen(url, data&#x3D;None, [timeout, ]*, cafile&#x3D;None, capath&#x3D;None, cadefault&#x3D;False, context&#x3D;None)</p><ol><li>url：url 地址。</li><li>data：发送到服务器的其他数据对象，默认为 No ne。</li><li>timeout：设置访问超时时间。</li><li>cafile 和 capath：cafile 为 CA 证书， 5.capath 为 CA 证书的路径，使用 HTTPS 需要用到。</li><li>cadefault：已经被弃用。</li><li>context：ssl.SSLContext类型，用来指定 SSL 设置。<br>python<br>from urllib.request import urlopen</li></ol><p>myURL &#x3D; urlopen(“<a href="https://www.runoob.com/&quot;">https://www.runoob.com/&quot;</a>)<br>print(myURL.read())</p><ol><li>read() 是读取整个网页内容，我们可以指定读取的长度：</li><li>readline() - 读取文件的一行内容</li><li>readlines() - 读取文件的全部内容，它会把读取的内容赋值给一个列表变量。  <blockquote><p>‘r’：只读。该文件必须已存在。<br>‘r+’：可读可写。该文件必须已存在，写为追加在文件内容末尾。<br>‘rb’：表示以二进制方式读取文件。该文件必须已存在。<br>‘w’：只写。打开即默认创建一个新文件，如果文件已存在，则覆盖写(即文件内原始数据会被新写入的数据清空覆盖)。<br>‘w+’：写读。打开创建新文件并写入数据，如果文件已存在，则覆盖写。<br>‘wb’：表示以二进制写方式打开，只能写文件， 如果文件不存在，创建该文件；如果文件已存在，则覆盖写。<br>‘a’：追加写。若打开的是已有文件则直接对已有文件操作，若打开文件不存在则创建新文件，只能执行写(追加在后面)，不能读。</p></blockquote></li></ol><p>import os<br>exists &#x3D; os.path.exists(“foo.txt”)<br>if exists:<br>    abspath &#x3D; os.path.abspath(“foo.txt”)<br>    print(abspath)<br>else:<br>    print(“文件不存在”)</p><p>**模拟头部信息<br>我们抓取网页一般需要对 headers（网页头信息）进行模拟，这时候需要使用到 urllib.request.Request 类：</p><p>class urllib.request.Request(url, data&#x3D;None, headers&#x3D;{}, origin_req_host&#x3D;None, unverifiable&#x3D;False, method&#x3D;None)<br>url：url 地址。<br>data：发送到服务器的其他数据对象，默认为 None。<br>headers：HTTP 请求的头部信息，字典格式。<br>origin_req_host：请求的主机地址，IP 或域名。<br>unverifiable：很少用整个参数，用于设置网页是否需要验证，默认是False。。<br>method：请求方法， 如 GET、POST、DELETE、PUT等。  </p><p>import urllib.request<br>import urllib.parse</p><p>url &#x3D; ‘<a href="https://www.runoob.com/?s=&#39;">https://www.runoob.com/?s=&#39;</a>  # 菜鸟教程搜索页面<br>keyword &#x3D; ‘Python 教程’<br>key_code &#x3D; urllib.request.quote(keyword)  # 对请求进行编码<br>url_all &#x3D; url+key_code<br>header &#x3D; {<br>    ‘User-Agent’:’Mozilla&#x2F;5.0 (X11; Fedora; Linux x86_64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;58.0.3029.110 Safari&#x2F;537.36’<br>}   #头部信息<br>request &#x3D; urllib.request.Request(url_all,headers&#x3D;header)<br>reponse &#x3D; urllib.request.urlopen(request).read()</p><p>fh &#x3D; open(“.&#x2F;urllib_test_runoob_search.html”,”wb”)    # 将文件写入到当前目录中<br>fh.write(reponse)<br>fh.close()</p><p><img src="https://www.runoob.com/wp-content/uploads/2021/04/6BD0D456-E929-4C11-9118-F09C85AEA427.jpg" alt="文件内容"></p><p><strong>urllib.error</strong><br>urllib.error 模块为 urllib.request 所引发的异常定义了异常类，基础异常类是 URLError。</p><p>urllib.error 包含了两个方法，URLError 和 HTTPError。</p><ol><li><p>URLError 是 OSError 的一个子类，用于处理程序在遇到问题时会引发此异常（或其派生的异常），包含的属性 reason 为引发异常的原因。</p></li><li><p>HTTPError 是 URLError 的一个子类，用于处理特殊 HTTP 错误例如作为认证请求的时候，包含的属性 code 为 HTTP 的状态码， reason 为引发异常的原因，headers 为导致 HTTPError 的特定 HTTP 请求的 HTTP 响应头。</p></li></ol><p>对不存在的网页抓取并处理异常:</p><p>import urllib.request<br>import urllib.error</p><p>myURL1 &#x3D; urllib.request.urlopen(“<a href="https://www.runoob.com/&quot;">https://www.runoob.com/&quot;</a>)<br>print(myURL1.getcode())   # 200</p><p>try:<br>    myURL2 &#x3D; urllib.request.urlopen(“<a href="https://www.runoob.com/no.html&quot;">https://www.runoob.com/no.html&quot;</a>)<br>except urllib.error.HTTPError as e:<br>    if e.code &#x3D;&#x3D; 404:<br>        print(404)   # 404</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line">**urllib.parse**  </span><br><span class="line">urllib.parse.urlparse(urlstring, scheme=&#x27;&#x27;, allow_fragments=True)</span><br><span class="line"></span><br><span class="line">from urllib.parse import urlparse</span><br><span class="line"></span><br><span class="line">o = urlparse(&quot;https://www.runoob.com/?s=python+%E6%95%99%E7%A8%8B&quot;)</span><br><span class="line">print(o)</span><br><span class="line">***********演示结果**********</span><br><span class="line">ParseResult(scheme=&#x27;https&#x27;, netloc=&#x27;www.runoob.com&#x27;, path=&#x27;/&#x27;, params=&#x27;&#x27;, query=&#x27;s=python+%E6%95%99%E7%A8%8B&#x27;, fragment=&#x27;&#x27;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">从结果可以看出，内容是一个元组，包含 6 个字符串：协议，位置，路径，参数，查询，判断。</span><br><span class="line"></span><br><span class="line">好家伙，索儿写的果真是抛砖引玉啊！  </span><br><span class="line">## 正则表达式  </span><br><span class="line">作用：正则可以使用限定符，匹配字符出现的次数，这样一来灵活度都高了  </span><br><span class="line">![重复限定符](https://img-blog.csdnimg.cn/20200506162325642.png?)  </span><br><span class="line">**正则表达式的特殊字符串**  </span><br><span class="line">![特殊字符串](https://img-blog.csdnimg.cn/20200506162325642.png?)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">我终于爬出来了！哈哈哈哈。  </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import re</span><br><span class="line">import time</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def ua_ck():</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    网站需要登录才能采集，需要从Network--Doc里复制User-Agent和Cookie，Cookie要转化为字典，否则会采集失败！！！！！</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    user_agent = &#123;</span><br><span class="line">        &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&#x27;&#125;</span><br><span class="line"></span><br><span class="line">    cookies = &#x27;ll=&quot;118202&quot;; bid=FsStcocWuPQ; _vwo_uuid_v2=D65179C81F8EE8041E5F8605041534542|e1ed6add019a5cf6cdb06398640e7fe6; ct=y; gr_user_id=43e3a769-ff1c-4abe-b1c3-f7d5b28082de; douban-fav-remind=1; viewed=&quot;20438158_10799082_3043970_35174681_26929955_3932365_26886337_27667378_33419041_33385402&quot;; push_doumail_num=0; push_noty_num=0; __utmc=30149280; __utmc=223695111; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1599712590%2C%22https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3D-946ZDrFNbdKZE0IGp73NUS3eCUaoTpabx75ZzZjM59T_FqZIgo-aeRfe2xfnu1o%26wd%3D%26eqid%3Da1f8e3670000ef4d000000065f59ad4b%22%5D; _pk_ses.100001.4cf6=*; __utma=30149280.1986063068.1597310055.1599704141.1599712590.74; __utmz=30149280.1599712590.74.69.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; __utmb=223695111.0.10.1599712590; __utma=223695111.1305332624.1597310055.1599704141.1599712590.43; __utmz=223695111.1599712590.43.40.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; ap_v=0,6.0; douban-profile-remind=1; __utmv=30149280.17799; __utmb=30149280.8.10.1599712590; dbcl2=&quot;223162585:V5BfOpq2kcs&quot;; ck=qnBB; _pk_id.100001.4cf6=991c66698d6e616d.1597310055.43.1599714528.1599704141.&#x27;</span><br><span class="line"></span><br><span class="line">    # Cookie转化为字典</span><br><span class="line">    cookies = cookies.split(&#x27;; &#x27;)</span><br><span class="line">    cookies_dict = &#123;&#125;</span><br><span class="line">    for i in cookies:</span><br><span class="line">        cookies_dict[i.split(&#x27;=&#x27;)[0]] = i.split(&#x27;=&#x27;)[1]</span><br><span class="line"></span><br><span class="line">    return user_agent, cookies_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_urls(n):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    n:页面数量，总共有25个页面</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    urls = []</span><br><span class="line">    num = (n-1)*25+1</span><br><span class="line">    for i in range(0, num, 25):</span><br><span class="line">        url = &#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27;.format(i)</span><br><span class="line">        urls.append(url)</span><br><span class="line"></span><br><span class="line">    return urls</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def get_movies_url(url, u_a, c_d):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    url：每一个页面的链接</span><br><span class="line">    u_a：User-Agent</span><br><span class="line">    c_d：cookies</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    html = requests.get(url,</span><br><span class="line">                        headers=u_a,  # 加载User-Agent</span><br><span class="line">                        cookies=c_d)  # 加载cookie</span><br><span class="line"></span><br><span class="line">    html.encoding = html.apparent_encoding  # 解决乱码的万金油方法</span><br><span class="line"></span><br><span class="line">    if html.status_code == 200:</span><br><span class="line">        print(&#x27;网页访问成功，代码：&#123;&#125;\n&#x27;.format(html.status_code))</span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(html.text, &#x27;html.parser&#x27;)  # 用 html.parser 来解析网页</span><br><span class="line">    items = soup.find(&#x27;ol&#x27;, class_=&#x27;grid_view&#x27;).find_all(&#x27;li&#x27;)</span><br><span class="line">    movies_url = []</span><br><span class="line"></span><br><span class="line">    for item in items:</span><br><span class="line">        # 电影链接</span><br><span class="line">        movie_href = item.find(&#x27;div&#x27;, class_=&#x27;hd&#x27;).find(&#x27;a&#x27;)[&#x27;href&#x27;]</span><br><span class="line">        movies_url.append(movie_href)</span><br><span class="line"></span><br><span class="line">    return movies_url</span><br><span class="line">    time.sleep(0.4)    # 设置时间间隔，0.4秒采集一次，避免频繁登录网页 </span><br><span class="line">    def get_movie_info(href, u_a, c_d):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    href：每一部电影的链接</span><br><span class="line">    u_a：User-Agent</span><br><span class="line">    c_d：cookies</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    html = requests.get(href,</span><br><span class="line">                        headers=u_a,</span><br><span class="line">                        cookies=c_d)</span><br><span class="line">    soup = BeautifulSoup(html.text, &#x27;html.parser&#x27;)  # 用 html.parser 来解析网页</span><br><span class="line">    item = soup.find(&#x27;div&#x27;, id=&#x27;content&#x27;)</span><br><span class="line"></span><br><span class="line">    movie = &#123;&#125;  # 新建字典，存放电影信息</span><br><span class="line"></span><br><span class="line">    # 电影名称</span><br><span class="line">    movie[&#x27;电影名称&#x27;] = item.h1.span.text</span><br><span class="line"></span><br><span class="line">    # 导演、类型、制片国家/地区、语言、上映时间、片长（部分电影这些信息不全，先全部采集，留待数据分析时处理）</span><br><span class="line">    movie[&#x27;电影其他信息&#x27;] = item.find(</span><br><span class="line">        &#x27;div&#x27;, id=&#x27;info&#x27;).text.replace(&#x27; &#x27;, &#x27;&#x27;).split(&#x27;\n&#x27;)</span><br><span class="line">    for i in movie[&#x27;电影其他信息&#x27;]:</span><br><span class="line">        if &#x27;:&#x27; in i:</span><br><span class="line">            movie[i.split(&#x27;:&#x27;)[0]] = i.split(&#x27;:&#x27;)[1]</span><br><span class="line">        else:</span><br><span class="line">            continue</span><br><span class="line"></span><br><span class="line">    # 豆瓣评分、评分人数</span><br><span class="line">    movie[&#x27;评分&#x27;] = item.find(&#x27;div&#x27;, id=&#x27;interest_sectl&#x27;).find(</span><br><span class="line">        &#x27;div&#x27;, class_=&#x27;rating_self clearfix&#x27;).find(&#x27;strong&#x27;, class_=&#x27;ll rating_num&#x27;).text</span><br><span class="line">    movie[&#x27;评分人数&#x27;] = item.find(&#x27;div&#x27;, id=&#x27;interest_sectl&#x27;).find(&#x27;div&#x27;, class_=&#x27;rating_self clearfix&#x27;).find(</span><br><span class="line">        &#x27;div&#x27;, class_=&#x27;rating_sum&#x27;).find(&#x27;span&#x27;, property=&#x27;v:votes&#x27;).text</span><br><span class="line"></span><br><span class="line">    return movie</span><br><span class="line">    time.sleep(0.4)  # 0.4秒采集一次，避免频繁登录网页</span><br><span class="line">    def main(n):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    n:页面数量，总共有10个页面</span><br><span class="line">    u_a：User-Agent</span><br><span class="line">    c_d：cookies</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    print(&#x27;开始采集数据，预计耗时2分钟&#x27;)</span><br><span class="line"></span><br><span class="line">    # 处理User-Agent和Cookie</span><br><span class="line">    login = ua_ck()</span><br><span class="line">    u_a = login[0]</span><br><span class="line">    c_d = login[1]</span><br><span class="line"></span><br><span class="line">    # 获取豆瓣top250每一页的链接，共10页</span><br><span class="line">    urls = get_urls(n)</span><br><span class="line">    print(&#x27;豆瓣10个网页链接已生成！！&#x27;)</span><br><span class="line"></span><br><span class="line">    # 获取每一页25部电影的链接，共250部</span><br><span class="line">    top250_urls = []</span><br><span class="line">    for url in urls:</span><br><span class="line">        result = get_movies_url(url, u_a, c_d)</span><br><span class="line">        top250_urls.extend(result)</span><br><span class="line">    print(&#x27;250部电影链接采集完成！！开始采集每部电影的详细信息(预计耗时5分钟).......&#x27;)</span><br><span class="line"></span><br><span class="line">    # 获取每一部电影的详细信息</span><br><span class="line">    top250_movie = []  # 储存每部电影的信息</span><br><span class="line">    error_href = []  # 储存采集错误的网址</span><br><span class="line"></span><br><span class="line">    for href in top250_urls:</span><br><span class="line">        try:</span><br><span class="line">            movie = get_movie_info(href, u_a, c_d)</span><br><span class="line">            top250_movie.append(movie)</span><br><span class="line">        except:</span><br><span class="line">            error_href.append(href)</span><br><span class="line">            print(&#x27;采集失败，失败网址是&#123;&#125;&#x27;.format(href))</span><br><span class="line"></span><br><span class="line">    print(&#x27;电影详细信息采集完成！！总共采集&#123;&#125;条数据&#x27;.format(len(top250_movie)))</span><br><span class="line">    return top250_movie, error_href</span><br><span class="line">result = main(10)</span><br><span class="line">df = pd.DataFrame(result[0])</span><br><span class="line">df.to_excel(&#x27;output1.xlsx&#x27;)</span><br></pre></td></tr></table></figure><p>加油！！！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>爬虫入门学习</title>
    <link href="http://example.com/2022/07/22/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2022/07/22/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/</id>
    <published>2022-07-22T11:46:46.000Z</published>
    <updated>2022-07-22T11:47:53.416Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>真正的成长，应该是注重健康的</p><p>爬虫一直以来，被人们以为是一项不尊重知识产权的技术，但是，就技术层面以来，他只是提供一种技术用来爬取结构化数据，这种数据可用于各种有用的应用程序，以及数据挖掘，信息处理或历史存档。因此，可以提供一种说法，就是技术无罪，我们可以通过此项技术提高生产力，产生更多有力的工具及资料，因此爬虫技术应该重新认识。  </p><blockquote><p>scrapy：是一个应用程序框架，用于对网站进行爬行和提取结构化数据，这些结构化数据可用于各种有用的应用程序，如数据挖掘、信息处理或历史存档。  </p></blockquote><hr><h1 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h1><h2 id="分析页面结构"><a href="#分析页面结构" class="headerlink" title="分析页面结构"></a>分析页面结构</h2><p><a href="http://quotes.toscrape.com/%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E7%BB%83%E4%B9%A0%E7%88%AC%E8%99%AB%E7%9A%84%E7%BD%91%E7%AB%99">http://quotes.toscrape.com/，是一个可以练习爬虫的网站</a><br><img src="https://img-blog.csdnimg.cn/20200401164052584.png" alt="爬虫小项目"><br>这是一个极为简单的页面，每个页面有十个class为quote的小卡片里面包含text，auther，tag。<br>每个页面有个下一页的按钮。 </p><p>url：<a href="http://quotes.toscrape.com/">http://quotes.toscrape.com/</a><br>由于结构过于繁琐，如果有点html结构基础，是可以判断整个页面的结构分布的，因此，直接开始。<br>每个quote是由三部分组成，文本，作者，标签。<br>具体的代码：    </p><pre><code>  import scrapy      class QuotesSpider(scrapy.Spider):      name = &#39;quotes&#39;     start_urls = [       &#39;http://quotes.toscrape.com/tag/humor/&#39;,    ]    def parse(self, response):    for quote in response.css(&#39;div.quote&#39;):        yield &#123;            &#39;author&#39;: quote.xpath(&#39;span/small/text()&#39;).get(),            &#39;text&#39;: quote.css(&#39;span.text::text&#39;).get(),        &#125;    next_page = response.css(&#39;li.next a::attr(&quot;href&quot;)&#39;).get()    if next_page is not None:        yield response.follow(next_page, self.parse)</code></pre><p>妈的 &gt;_&lt; 好难啊  </p><h2 id="分析代码"><a href="#分析代码" class="headerlink" title="分析代码"></a>分析代码</h2><blockquote><p>def parse(self, response):</p></blockquote><hr><p>这个 parse() 方法通常解析response，将抓取的数据提取为dict，并查找新的URL以跟踪和创建新的请求  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">&#x27;author&#x27;</span>: quote.xpath(<span class="string">&#x27;span/small/text()&#x27;</span>).get(),</span><br><span class="line">            <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    next_page = response.css(<span class="string">&#x27;li.next a::attr(&quot;href&quot;)&#x27;</span>).get()</span><br><span class="line">    <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">yield</span> response.follow(next_page, self.parse)</span><br><span class="line">```  </span><br><span class="line">使用CSS选择器循环 div.quote 元素，然后使用css选择器或者xpath生成一个包含 quote的作者、文本的列表  </span><br><span class="line">css选择器的使用无缝对接css中我们对选择器的认知</span><br><span class="line">xpath是官方推荐我们使用的</span><br><span class="line">查找到下一页的链接，并使用它继续调度 parse() 方法</span><br><span class="line"><span class="comment">## scrapy spider  </span></span><br><span class="line">&gt;scrapy startproject xxx</span><br><span class="line">---</span><br><span class="line">实现scrapy spider</span><br><span class="line"><span class="comment">### 第一步 创建一个新的scrapy项目  </span></span><br></pre></td></tr></table></figure><p>scrapy startproject xxx</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">### 第二步 定义继承spider的类并定义初始请求  </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;quotes&quot;</span><br><span class="line">    start_urls = [</span><br><span class="line">        &#x27;http://quotes.toscrape.com/page/1/&#x27;,</span><br><span class="line">        &#x27;http://quotes.toscrape.com/page/2/&#x27;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        page = response.url.split(&quot;/&quot;)[-2] #根据上面的链接提取分页,如：/page/1/，提取到的就是：1</span><br><span class="line">        filename = &#x27;quotes-%s.html&#x27; % page #拼接文件名，如果是第一页，最终文件名便是：newpage-1.html</span><br><span class="line">        with open(filename, &#x27;wb&#x27;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(&#x27;Saved file %s&#x27; % filename)</span><br></pre></td></tr></table></figure><p>还要定义属性和方法</p><p>name ：标识Spider。它在一个项目中必须是唯一的，也就是说，不能为不同的Spider设置相同的名称。</p><p>start_urls：具有URL列表的类属性</p><p>parse() ：将被调用来处理这些URL的每个请求。parse() 是Scrapy的默认回调方法，对没有显式分配回调的请求调用该方法。</p><p>parse() 方法通常解析响应，将抓取的数据提取为dict，并查找新的URL以跟踪和创建新的请求。  </p><p><em><strong>scrapy crawl quotes</strong></em><br>妈的，学会了，不想写了<del>_</del>。  </p><h3 id="scarpy-shell"><a href="#scarpy-shell" class="headerlink" title="scarpy shell"></a>scarpy shell</h3><p>终端输入：</p><blockquote><p>scrapy shell url  </p></blockquote><p>response.css()<br>response.css(‘xxx’) 返回的是一个类似列表的对象：SelectorList，<br>data中存储的是提取到的标签，<br>xpath中存储的XPath表达式，实际上CSS选择器是在后台转换为XPath表达式的<br>允许你进一步的细化选择和提取数据<br>response.css(‘xxx::text’) 与上面方法不同之处在于，data中存储的是提取到的标签的文本<br>response.css(‘xxx’).getall()、response.css(‘xxx::text’).getall()<br>getall() 方法，返回的是一个列表，<br>列表中的值是 SelectorList 中的 data 值<br>一般而言，选择器返回的结果不止一个，getall() 方法 提取全部内容  </p><p>response.xpath这个不写了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>quote = response.css(<span class="string">&quot;div.quote&quot;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = quote.css(<span class="string">&quot;span.text::text&quot;</span>).get()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text</span><br><span class="line"><span class="string">&#x27;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>author = quote.css(<span class="string">&quot;small.author::text&quot;</span>).get()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>author</span><br><span class="line"><span class="string">&#x27;Albert Einstein&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags = quote.css(<span class="string">&quot;div.tags a.tag::text&quot;</span>).getall()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags</span><br><span class="line">[<span class="string">&#x27;change&#x27;</span>, <span class="string">&#x27;deep-thoughts&#x27;</span>, <span class="string">&#x27;thinking&#x27;</span>, <span class="string">&#x27;world&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="存取json文件"><a href="#存取json文件" class="headerlink" title="存取json文件"></a>存取json文件</h3><p>在项目顶层目录下输入scrapy crawl quotes -o quotes.json</p><p>Scrapy 会生成quotes.json文件，并将爬取到的数据放到quotes.json文件中</p><h3 id="具体栗子"><a href="#具体栗子" class="headerlink" title="具体栗子"></a>具体栗子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuthorSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;author&#x27;</span></span><br><span class="line"></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        author_page_links = response.css(<span class="string">&#x27;.author + a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(author_page_links, self.parse_author)</span><br><span class="line"></span><br><span class="line">        pagination_links = response.css(<span class="string">&#x27;li.next a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(pagination_links, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_author</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">extract_with_css</span>(<span class="params">query</span>):</span><br><span class="line">            <span class="keyword">return</span> response.css(query).get(default=<span class="string">&#x27;&#x27;</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: extract_with_css(<span class="string">&#x27;h3.author-title::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;birthdate&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-born-date::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;birthlocation&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-born-location::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;bio&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-description::text&#x27;</span>),</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>就写这么多吧。<br>本文参考CSDN博主索儿呀，索儿老师整理十分仔细，我写这个是为了记录学习爬虫的相关经历，如有冒犯，我会苏珊&gt;__&lt;<br><em><strong><a href="https://zhangguohao.blog.csdn.net/article/details/105245153">https://zhangguohao.blog.csdn.net/article/details/105245153</a></strong></em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>关于ajax的使用说明</title>
    <link href="http://example.com/2022/07/17/%E5%85%B3%E4%BA%8Eajax%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"/>
    <id>http://example.com/2022/07/17/%E5%85%B3%E4%BA%8Eajax%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</id>
    <published>2022-07-17T07:43:22.000Z</published>
    <updated>2022-07-17T09:56:51.170Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>今天看关于ajax技术，这种技术本身是不难的，但是由于技术粗糙，好久没练过了，导致项目不足够了解，出现了许多问题，也有了整理这个的必要<br>呜呜呜</p><blockquote><p>AJAX &#x3D; Asynchronous JavaScript and XML（异步的 JavaScript 和 XML） AJAX 不是新的编程语言，而是一种使用现有标准的新方法。AJAX 最大的优点是在不重新加载整个页面的情况下，可以与服务器交换数据并更新部分网页内容。AJAX 不需要任何浏览器插件，但需要用户允许JavaScript在浏览器上执行。  </p></blockquote><h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1><hr><p>接着，开始编写具体的功能。  </p><p>**接着，最操蛋的事情来了，maven的pom.xml一直配置不好，速度奇慢，因此，搜索到了一种方法那就是！利用阿里云的镜像地址，速度飞起！</p><h2 id="打开-x2F-创建-settings-xml"><a href="#打开-x2F-创建-settings-xml" class="headerlink" title="打开&#x2F;创建 settings.xml"></a>打开&#x2F;创建 settings.xml</h2><p><img src="/../img/10.png" alt="这是图片"></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;      xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;&lt;mirrors&gt;    &lt;mirror&gt;        &lt;id&gt;alimaven&lt;/id&gt;        &lt;name&gt;aliyun maven&lt;/name&gt;        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;    &lt;/mirror&gt;    &lt;mirror&gt;        &lt;id&gt;uk&lt;/id&gt;        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;        &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt;        &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt;    &lt;/mirror&gt;    &lt;mirror&gt;        &lt;id&gt;CN&lt;/id&gt;        &lt;name&gt;OSChina Central&lt;/name&gt;        &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;    &lt;/mirror&gt;    &lt;mirror&gt;        &lt;id&gt;nexus&lt;/id&gt;        &lt;name&gt;internal nexus repository&lt;/name&gt;        &lt;url&gt;http://repo.maven.apache.org/maven2&lt;/url&gt;        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;    &lt;/mirror&gt;&lt;/mirrors&gt;</code></pre></settings>  如此，便可以加快相关的部署及下载速度，同时，使用相关的jquery，我使用的是网址，并没有进行相关的maven配置。    <pre><code>&lt;script src=&quot;https://ajax.aspnetcdn.com/ajax/jquery/jquery-3.5.1.min.js&quot;&gt;&lt;/script&gt; </code></pre><h2 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h2><p>具体的设想是设计一个输入框失去焦点的事件，即onblur()事件，对于页面进行局部刷新，实现相关的ajax作用  </p><blockquote><p>用户名<br>    <input type="text" id="txtname" onblur="an()">  </p></blockquote><pre><code>function an()&#123;    //请求将文本框输入的值发给服务器，    // 接受服务器的值    $.ajax(&#123;        url:&quot;$&#123;pageContext.request.contextPath&#125;/ajax/a1&quot;,        data:&#123;&quot;name&quot;:$(&quot;#txtname&quot;).val()&#125;,        success:function (data,status)&#123;            console.log(data)            alert(status)        &#125;    &#125;);&#125;</code></pre><p>然后实现当输入框内容为admin时，控制台显示为true，当输入框为其他数时，则为false。</p><p><img src="/../img/11.png" alt="不为admin时"></p><blockquote><p>Failed to load resource: the server responded with a status of 404 ()此类报错为正常错误，无须担心</p></blockquote><hr><p><img src="/../img/12.png" alt="为admin"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>心理学：人为什么会想要攻击别人？</title>
    <link href="http://example.com/2022/07/13/project/"/>
    <id>http://example.com/2022/07/13/project/</id>
    <published>2022-07-13T13:27:33.000Z</published>
    <updated>2022-07-13T14:15:08.359Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>为什么别人喜欢在网络上攻击他人，从开始的蔡徐坤篮球，到现在的易烊千玺，虽然我也不停的伤害别人，但是就个人至群体，以现象推出人性。<br><img src="/../img/1.png" alt="1"></p><h1 id="伤害别人无疑是丢人的"><a href="#伤害别人无疑是丢人的" class="headerlink" title="伤害别人无疑是丢人的"></a>伤害别人无疑是丢人的</h1><p>我们把攻击行为定义为意图伤害他人的身体行为或者言语行为。</p><p>这一定义排除了车祸、牙科治疗和人行道上的碰撞，但包括打耳光，当面侮辱，甚至说风凉话。该定义涵盖了两种不同的攻击行为。</p><p>当动物发怒时，它们在展示典型的社会性攻击行为；而当掠食者潜行在猎物之后时，它们表现的是静息的攻击行为。社会性和静息攻击行为分属不同的脑区。</p><h2 id="心理学将其攻击行为设为两种"><a href="#心理学将其攻击行为设为两种" class="headerlink" title="心理学将其攻击行为设为两种"></a>心理学将其攻击行为设为两种</h2><p><img src="/../img/2.png" alt="2"></p><h3 id="敌意性"><a href="#敌意性" class="headerlink" title="敌意性"></a>敌意性</h3><p>敌意性攻击行为由愤怒引起，以伤害为目的。</p><p>工具性攻击行为只是把伤害作为达到其他目的的一种手段。</p><p>大多数恐怖活动属于工具性攻击，谋杀大多是敌意性的。关于攻击行为的三种主要理论</p><p>与弗洛伊德和洛伦兹关系密切的本能观点认为攻击性的能量会在体内不断积累，就像水在大坝后积聚一样。</p><p>虽然这种观点很少有直接证据的支持，但攻击行为确实受到遗传、血液化学成分和大脑等生物学因素的影响。<br><img src="/../img/3.png" alt="3"></p><h3 id="挫折感"><a href="#挫折感" class="headerlink" title="挫折感"></a>挫折感</h3><p>那是一个暖和的夜晚，在两个小时的认真学习之后，你觉得又累又渴，于是你向朋友借了一些零钱，走向最近的一个自动售货机。你扫码购买一瓶饮料，迫不及待地想要喝一口冰凉爽口的可乐。</p><p>但是，当你按下选择的按钮时，售货机却完全没有反应。你又按了一次，然后按下了把钱退出的按钮，机器仍然毫无动静。你用力地敲打着按钮，然后用拳头捶它们。最后你晃动、敲打售货机，可是没反应。你两手空空非常生气地摔门回了寝室。此时，你的室友是不是特别小心的对待你呢？那时的你是否更容易说出一些伤人的话语，甚至做出一些伤害性的事情呢？</p><p>“挫折总会导致某种形式的攻击行为。”</p><p>这里的挫折指的是，任何阻碍我们实现目标的事物(比如那个出现故障的自动售货机)。</p><p>当我们达到一个目标的动机非常强烈，当我们预期得到满意的结果，却在行动过程中遇到障碍时，挫折便产生了。</p><p>攻击的能量并非直接朝挫折源释放。我们学会克制直接的报复，特别当别人会对这种行为表示反对或者进行惩罚时；</p><p>相反，我们会把我们的敌意转移到一些安全的目标上。</p><p>一则古老的故事为“转移”做了很好的诠释：</p><p>一个被老板羞辱的男人回家以后大声斥责他的妻子，妻子只好向儿子咆哮，儿子只能踢狗解气，而狗则把来送信的邮递员咬了一口。</p><p>在实验情境和现实生活中，当新的目标与挫折源有相似之处，并且稍稍刺激了攻击能量的释放时，攻击的转移最容易发生。</p><p>后续研究发现，“相对剥夺”也会造成挫折。</p><p>当我们把自己和他人进行比较时，我们的挫折感就会变得较为复杂。</p><p>工人的幸福感取决于和同一条工作线上其他人相比他们获得的报酬是否公平。</p><p>提高城市警察的工资水平虽然可以暂时提高他们的士气，却可能同时降低该市消防员的士气。</p><p>这种感觉称为相对剥夺。它可以预测少数人群体在感觉到不平等待遇时会做出什么样的反应。</p><p>相对剥夺同样可以解释，为什么在贫富差距大的国家和社会里，人们的幸福感较低而犯罪率较高。</p><p>在高速现代化的国家里，随着城市化程度和人们的文化水平的提高。他们对物质生活的可能的前景越来越敏感。但富裕群体通常只能较慢地扩展。</p><p>因此，人们的期望与实际所得之间的差距越来越大，这使得他们挫折感变得更为强烈。一旦人们的期望超过了现实生活，挫折和政治攻击行为也依然会逐步增加。</p><p>第三，社会学习理论认为我们的攻击行为是习得的。通过亲身经历和观察别人的成功，我们会习得攻击行为的好处。</p><p>社会学习使家庭、亚文化和大众媒体都能对攻击行为产生重要的影响。</p><p>心理学：在什么情况下人会有暴力？如何减少攻击行为？<br><img src="/../img/4.png" alt="4"></p><h2 id="攻击行为理论"><a href="#攻击行为理论" class="headerlink" title="攻击行为理论"></a>攻击行为理论</h2><blockquote><p>攻击行为（aggression）：意图伤害他人的身体行为或者言语行为。敌意性攻击行为（hostile aggression）：由愤怒引起，以伤害为目的。（如：谋杀）  </p></blockquote><blockquote><p>工具性攻击行为（instrumental aggression）：只是把伤害作为达到其他目的的一种手段。（如：战争、恐怖活动等）</p></blockquote><p>关于攻击行为有三种主要理论。与弗洛伊德和洛伦兹关系密切的本能观点认为攻击性的能量会在体内不断积累，就像水在大坝后集聚一样。虽然这种观点很少有直接证据的支持，但攻击行为确实受到遗传、血液化学成分和大脑等生物学因素的影响。</p><p>【结果发现：未受过虐待的杀人犯的前额叶激活水平比正常人低14%，反社会者的前额叶则比正常人小15%，而前额叶被认为是对于攻击性行为有关的脑区进行紧急抑制的。</p><p>遗传因素影响神经系统对暴力线索的敏感性。在灵长类动物和人类中，攻击性天然有着较大的多样性。我们的气质（即我们的反应性和反应强度）部分是与生俱来的，同时也受交感神经系统反应性的影响。</p><p>攻击行为是由一种能够改变神经递质平衡的基因和童年时期的受虐待经历共同决定的。攻击性和反社会行为并非单纯地只受“不良”基因或是“不良”环境的影响；相反，基因会使某些儿童对虐待更敏感，反应更强烈。先天和后天因素是互相影响的。</p><p>血液中的化学成分同样可以影响神经系统对攻击性刺激的敏感性。酒精可以降低人们的自我觉知和考虑后果的能力，进而增加暴力行为发生的可能。酒精使人们的个性弱化，降低我们的抑制能力。暴力行为与雄性激素即睾丸激素也有关系：在正常的青少年和成年人中，那些睾丸激素水平高的人更容易出现不良行为，使用致麻醉品以及对挑衅产生攻击性回应。</p><p>暴力行为另一个常见的元凶是神经递质5-羟色胺的缺乏，在控制冲动的额叶区有许多接收器。社会经济地位比较低的人5-羟色胺水平往往也比较低。进化心理学家认为，这也许是一种自然的反应，这种状态使他们敢于承担风险去增进他们的利益和地位。】</p><p>第二种观点认为是挫折产生了愤怒和敌意，如果存在攻击性的线索，这种愤怒就可能激起攻击行为。挫折感不仅来自剥夺本身，还来自期望和现实之间的差距，这称之为相对剥夺（relative deprivation）。</p><p>社会学习理论认为我们的攻击行为是习得的。通过亲身经历和观察别人的成功，我们会习得攻击行为的好处。社会学习使家庭、亚文化和大众媒体都能对攻击行为产生重要的影响。<br><img src="/../img/5.png" alt="5"></p><h2 id="如何减少攻击"><a href="#如何减少攻击" class="headerlink" title="如何减少攻击"></a>如何减少攻击</h2><p>与宣泄假设相反，发泄攻击更多地引发攻击的产生，而非减少进一步的攻击。</p><p>社会学习观点建议通过消除引发攻击的因素来控制它——通过减少令人厌恶的刺激，奖励和塑造非攻击行为，和产生于攻击行为不一致的反应。<br>总而言之，归结下来只有两点..</p><p><em>咳咳，不是这两点..,是以下的两个方面</em></p><p><strong>弘扬社会正气，召唤公共良序</strong> </p><p><strong>深入了解事实，确定当前自己的切身需求以及个人能力，不做超乎能力范围之类的事，同时，做人应该具备相当的觉悟，放弃对对立事情的所有幻想。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>demo</title>
    <link href="http://example.com/2022/07/12/demo/"/>
    <id>http://example.com/2022/07/12/demo/</id>
    <published>2022-07-12T09:05:23.000Z</published>
    <updated>2022-07-12T09:21:27.562Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>策为人，美姿颜，好笑语，性阔达听受，善于用人，是以士民见者，莫不尽心，乐为致死。策时年少，虽有位号，而士民皆呼为孙郎。百姓闻孙郎至，皆失魂魄。 英气杰济，猛锐冠世，览奇取异，志陵中夏，轻佻果躁 。</p><h2 id="孙策"><a href="#孙策" class="headerlink" title="孙策"></a>孙策</h2><h3 id="曹操对孙策的评价"><a href="#曹操对孙策的评价" class="headerlink" title="曹操对孙策的评价"></a>曹操对孙策的评价</h3><p>由于本人最近沉迷三国演义，在网上搜集了相当多的文献，尤其本人对孙策，孙伯符极为敬仰，因此，创建此篇帖子。<br>曹操对孙策有两次评价，一次在与刘备的青梅煮酒论英雄中谈及，当时刘备对孙策极为推崇，认为他是尽得六军十八州，是为当世之豪杰，然曹操却认为：孙策藉父之名，非英雄也</p><p>然官渡之战之前，却大为改观:猘儿，谓难与争锋。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>demo</title>
    <link href="http://example.com/2022/07/12/%E7%88%AC%E8%99%AB/"/>
    <id>http://example.com/2022/07/12/%E7%88%AC%E8%99%AB/</id>
    <published>2022-07-12T09:05:23.000Z</published>
    <updated>2022-07-22T11:43:59.400Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>玩一玩爬虫咯</p><p>爬虫一直以来，被人们以为是一项不尊重知识产权的技术，但是，就技术层面以来，他只是提供一种技术用来爬取结构化数据，这种数据可用于各种有用的应用程序，以及数据挖掘，信息处理或历史存档。因此，可以提供一种说法，就是技术无罪，我们可以通过此项技术提高生产力，产生更多有力的工具及资料，因此爬虫技术应该重新认识。  </p><blockquote><p>scrapy：是一个应用程序框架，用于对网站进行爬行和提取结构化数据，这些结构化数据可用于各种有用的应用程序，如数据挖掘、信息处理或历史存档。  </p></blockquote><hr><h1 id="入门爬虫"><a href="#入门爬虫" class="headerlink" title="入门爬虫"></a>入门爬虫</h1><h2 id="分析页面结构"><a href="#分析页面结构" class="headerlink" title="分析页面结构"></a>分析页面结构</h2><p><a href="http://quotes.toscrape.com/%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E7%BB%83%E4%B9%A0%E7%88%AC%E8%99%AB%E7%9A%84%E7%BD%91%E7%AB%99">http://quotes.toscrape.com/，是一个可以练习爬虫的网站</a><br><img src="https://img-blog.csdnimg.cn/20200401164052584.png" alt="爬虫小项目"><br>这是一个极为简单的页面，每个页面有十个class为quote的小卡片里面包含text，auther，tag。<br>每个页面有个下一页的按钮。 </p><p>url：<a href="http://quotes.toscrape.com/">http://quotes.toscrape.com/</a><br>由于结构过于繁琐，如果有点html结构基础，是可以判断整个页面的结构分布的，因此，直接开始。<br>每个quote是由三部分组成，文本，作者，标签。<br>具体的代码：    </p><pre><code>  import scrapy      class QuotesSpider(scrapy.Spider):      name = &#39;quotes&#39;     start_urls = [       &#39;http://quotes.toscrape.com/tag/humor/&#39;,    ]    def parse(self, response):    for quote in response.css(&#39;div.quote&#39;):        yield &#123;            &#39;author&#39;: quote.xpath(&#39;span/small/text()&#39;).get(),            &#39;text&#39;: quote.css(&#39;span.text::text&#39;).get(),        &#125;    next_page = response.css(&#39;li.next a::attr(&quot;href&quot;)&#39;).get()    if next_page is not None:        yield response.follow(next_page, self.parse)</code></pre><p>妈的 &gt;_&lt; 好难啊  </p><h2 id="分析代码"><a href="#分析代码" class="headerlink" title="分析代码"></a>分析代码</h2><blockquote><p>def parse(self, response):</p></blockquote><hr><p>这个 parse() 方法通常解析response，将抓取的数据提取为dict，并查找新的URL以跟踪和创建新的请求  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">&#x27;author&#x27;</span>: quote.xpath(<span class="string">&#x27;span/small/text()&#x27;</span>).get(),</span><br><span class="line">            <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    next_page = response.css(<span class="string">&#x27;li.next a::attr(&quot;href&quot;)&#x27;</span>).get()</span><br><span class="line">    <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">yield</span> response.follow(next_page, self.parse)</span><br><span class="line">```  </span><br><span class="line">使用CSS选择器循环 div.quote 元素，然后使用css选择器或者xpath生成一个包含 quote的作者、文本的列表  </span><br><span class="line">css选择器的使用无缝对接css中我们对选择器的认知</span><br><span class="line">xpath是官方推荐我们使用的</span><br><span class="line">查找到下一页的链接，并使用它继续调度 parse() 方法</span><br><span class="line"><span class="comment">## scrapy spider  </span></span><br><span class="line">&gt;scrapy startproject xxx</span><br><span class="line">---</span><br><span class="line">实现scrapy spider</span><br><span class="line"><span class="comment">### 第一步 创建一个新的scrapy项目  </span></span><br></pre></td></tr></table></figure><p>scrapy startproject xxx</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">### 第二步 定义继承spider的类并定义初始请求  </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;quotes&quot;</span><br><span class="line">    start_urls = [</span><br><span class="line">        &#x27;http://quotes.toscrape.com/page/1/&#x27;,</span><br><span class="line">        &#x27;http://quotes.toscrape.com/page/2/&#x27;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        page = response.url.split(&quot;/&quot;)[-2] #根据上面的链接提取分页,如：/page/1/，提取到的就是：1</span><br><span class="line">        filename = &#x27;quotes-%s.html&#x27; % page #拼接文件名，如果是第一页，最终文件名便是：newpage-1.html</span><br><span class="line">        with open(filename, &#x27;wb&#x27;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(&#x27;Saved file %s&#x27; % filename)</span><br></pre></td></tr></table></figure><p>还要定义属性和方法</p><p>name ：标识Spider。它在一个项目中必须是唯一的，也就是说，不能为不同的Spider设置相同的名称。</p><p>start_urls：具有URL列表的类属性</p><p>parse() ：将被调用来处理这些URL的每个请求。parse() 是Scrapy的默认回调方法，对没有显式分配回调的请求调用该方法。</p><p>parse() 方法通常解析响应，将抓取的数据提取为dict，并查找新的URL以跟踪和创建新的请求。  </p><p><em><strong>scrapy crawl quotes</strong></em><br>妈的，学会了，不想写了<del>_</del>。  </p><h3 id="scarpy-shell"><a href="#scarpy-shell" class="headerlink" title="scarpy shell"></a>scarpy shell</h3><p>终端输入：</p><blockquote><p>scrapy shell url  </p></blockquote><p>response.css()<br>response.css(‘xxx’) 返回的是一个类似列表的对象：SelectorList，<br>data中存储的是提取到的标签，<br>xpath中存储的XPath表达式，实际上CSS选择器是在后台转换为XPath表达式的<br>允许你进一步的细化选择和提取数据<br>response.css(‘xxx::text’) 与上面方法不同之处在于，data中存储的是提取到的标签的文本<br>response.css(‘xxx’).getall()、response.css(‘xxx::text’).getall()<br>getall() 方法，返回的是一个列表，<br>列表中的值是 SelectorList 中的 data 值<br>一般而言，选择器返回的结果不止一个，getall() 方法 提取全部内容  </p><p>response.xpath这个不写了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>quote = response.css(<span class="string">&quot;div.quote&quot;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = quote.css(<span class="string">&quot;span.text::text&quot;</span>).get()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text</span><br><span class="line"><span class="string">&#x27;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>author = quote.css(<span class="string">&quot;small.author::text&quot;</span>).get()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>author</span><br><span class="line"><span class="string">&#x27;Albert Einstein&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags = quote.css(<span class="string">&quot;div.tags a.tag::text&quot;</span>).getall()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags</span><br><span class="line">[<span class="string">&#x27;change&#x27;</span>, <span class="string">&#x27;deep-thoughts&#x27;</span>, <span class="string">&#x27;thinking&#x27;</span>, <span class="string">&#x27;world&#x27;</span>]</span><br></pre></td></tr></table></figure><h3 id="存取json文件"><a href="#存取json文件" class="headerlink" title="存取json文件"></a>存取json文件</h3><p>在项目顶层目录下输入scrapy crawl quotes -o quotes.json</p><p>Scrapy 会生成quotes.json文件，并将爬取到的数据放到quotes.json文件中</p><h3 id="具体栗子"><a href="#具体栗子" class="headerlink" title="具体栗子"></a>具体栗子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuthorSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;author&#x27;</span></span><br><span class="line"></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        author_page_links = response.css(<span class="string">&#x27;.author + a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(author_page_links, self.parse_author)</span><br><span class="line"></span><br><span class="line">        pagination_links = response.css(<span class="string">&#x27;li.next a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(pagination_links, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_author</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">extract_with_css</span>(<span class="params">query</span>):</span><br><span class="line">            <span class="keyword">return</span> response.css(query).get(default=<span class="string">&#x27;&#x27;</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: extract_with_css(<span class="string">&#x27;h3.author-title::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;birthdate&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-born-date::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;birthlocation&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-born-location::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;bio&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-description::text&#x27;</span>),</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>就写这么多吧。<br>本文参考CSDN博主索儿呀，索儿老师整理十分仔细，我写这个是为了记录学习爬虫的相关经历，如有冒犯，我会苏珊&gt;__&lt;<br><em><strong><a href="https://zhangguohao.blog.csdn.net/article/details/105245153">https://zhangguohao.blog.csdn.net/article/details/105245153</a></strong></em></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>we have a great day</title>
    <link href="http://example.com/2022/07/11/hello-world/"/>
    <id>http://example.com/2022/07/11/hello-world/</id>
    <published>2022-07-11T07:15:32.815Z</published>
    <updated>2022-07-12T03:18:18.660Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
</feed>
