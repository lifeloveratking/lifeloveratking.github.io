<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>关于ajax的使用说明</title>
    <url>/2022/07/17/%E5%85%B3%E4%BA%8Eajax%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>今天看关于ajax技术，这种技术本身是不难的，但是由于技术粗糙，好久没练过了，导致项目不足够了解，出现了许多问题，也有了整理这个的必要<br>呜呜呜</p>
<blockquote>
<p>AJAX &#x3D; Asynchronous JavaScript and XML（异步的 JavaScript 和 XML） AJAX 不是新的编程语言，而是一种使用现有标准的新方法。AJAX 最大的优点是在不重新加载整个页面的情况下，可以与服务器交换数据并更新部分网页内容。AJAX 不需要任何浏览器插件，但需要用户允许JavaScript在浏览器上执行。  </p>
</blockquote>
<h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1><hr>
<p>接着，开始编写具体的功能。  </p>
<p>**接着，最操蛋的事情来了，maven的pom.xml一直配置不好，速度奇慢，因此，搜索到了一种方法那就是！利用阿里云的镜像地址，速度飞起！</p>
<h2 id="打开-x2F-创建-settings-xml"><a href="#打开-x2F-创建-settings-xml" class="headerlink" title="打开&#x2F;创建 settings.xml"></a>打开&#x2F;创建 settings.xml</h2><p><img src="/../img/10.png" alt="这是图片"></p>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;
      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
      xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;
&lt;mirrors&gt;
    &lt;mirror&gt;
        &lt;id&gt;alimaven&lt;/id&gt;
        &lt;name&gt;aliyun maven&lt;/name&gt;
        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;
        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;
    &lt;/mirror&gt;

    &lt;mirror&gt;
        &lt;id&gt;uk&lt;/id&gt;
        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;
        &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt;
        &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt;
    &lt;/mirror&gt;

    &lt;mirror&gt;
        &lt;id&gt;CN&lt;/id&gt;
        &lt;name&gt;OSChina Central&lt;/name&gt;
        &lt;url&gt;http://maven.oschina.net/content/groups/public/&lt;/url&gt;
        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;
    &lt;/mirror&gt;

    &lt;mirror&gt;
        &lt;id&gt;nexus&lt;/id&gt;
        &lt;name&gt;internal nexus repository&lt;/name&gt;
        &lt;url&gt;http://repo.maven.apache.org/maven2&lt;/url&gt;
        &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;
    &lt;/mirror&gt;
&lt;/mirrors&gt;
</code></pre>
</settings>  
如此，便可以加快相关的部署及下载速度，同时，使用相关的jquery，我使用的是网址，并没有进行相关的maven配置。    

<pre><code>&lt;script src=&quot;https://ajax.aspnetcdn.com/ajax/jquery/jquery-3.5.1.min.js&quot;&gt;&lt;/script&gt; 
</code></pre>
<h2 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h2><p>具体的设想是设计一个输入框失去焦点的事件，即onblur()事件，对于页面进行局部刷新，实现相关的ajax作用  </p>
<blockquote>
<p>用户名<br>    <input type="text" id="txtname" onblur="an()">  </p>
</blockquote>
<pre><code>function an()&#123;
    //请求将文本框输入的值发给服务器，
    // 接受服务器的值
    $.ajax(&#123;
        url:&quot;$&#123;pageContext.request.contextPath&#125;/ajax/a1&quot;,
        data:&#123;&quot;name&quot;:$(&quot;#txtname&quot;).val()&#125;,
        success:function (data,status)&#123;
            console.log(data)
            alert(status)
        &#125;
    &#125;);

&#125;
</code></pre>
<p>然后实现当输入框内容为admin时，控制台显示为true，当输入框为其他数时，则为false。</p>
<p><img src="/../img/11.png" alt="不为admin时"></p>
<blockquote>
<p>Failed to load resource: the server responded with a status of 404 ()此类报错为正常错误，无须担心</p>
</blockquote>
<hr>
<p><img src="/../img/12.png" alt="为admin"></p>
]]></content>
  </entry>
  <entry>
    <title>爬虫入门学习</title>
    <url>/2022/07/22/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>真正的成长，应该是注重健康的</p>
<p>爬虫一直以来，被人们以为是一项不尊重知识产权的技术，但是，就技术层面以来，他只是提供一种技术用来爬取结构化数据，这种数据可用于各种有用的应用程序，以及数据挖掘，信息处理或历史存档。因此，可以提供一种说法，就是技术无罪，我们可以通过此项技术提高生产力，产生更多有力的工具及资料，因此爬虫技术应该重新认识。  </p>
<blockquote>
<p>scrapy：是一个应用程序框架，用于对网站进行爬行和提取结构化数据，这些结构化数据可用于各种有用的应用程序，如数据挖掘、信息处理或历史存档。  </p>
</blockquote>
<hr>
<h1 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h1><h2 id="分析页面结构"><a href="#分析页面结构" class="headerlink" title="分析页面结构"></a>分析页面结构</h2><p><a href="http://quotes.toscrape.com/%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E7%BB%83%E4%B9%A0%E7%88%AC%E8%99%AB%E7%9A%84%E7%BD%91%E7%AB%99">http://quotes.toscrape.com/，是一个可以练习爬虫的网站</a><br><img src="https://img-blog.csdnimg.cn/20200401164052584.png" alt="爬虫小项目"><br>这是一个极为简单的页面，每个页面有十个class为quote的小卡片里面包含text，auther，tag。<br>每个页面有个下一页的按钮。 </p>
<p>url：<a href="http://quotes.toscrape.com/">http://quotes.toscrape.com/</a><br>由于结构过于繁琐，如果有点html结构基础，是可以判断整个页面的结构分布的，因此，直接开始。<br>每个quote是由三部分组成，文本，作者，标签。<br>具体的代码：    </p>
<pre><code>  import scrapy

      class QuotesSpider(scrapy.Spider):
      name = &#39;quotes&#39;
     start_urls = [
       &#39;http://quotes.toscrape.com/tag/humor/&#39;,
    ]

    def parse(self, response):
    for quote in response.css(&#39;div.quote&#39;):
        yield &#123;
            &#39;author&#39;: quote.xpath(&#39;span/small/text()&#39;).get(),
            &#39;text&#39;: quote.css(&#39;span.text::text&#39;).get(),
        &#125;

    next_page = response.css(&#39;li.next a::attr(&quot;href&quot;)&#39;).get()
    if next_page is not None:
        yield response.follow(next_page, self.parse)
</code></pre>
<p>妈的 &gt;_&lt; 好难啊  </p>
<h2 id="分析代码"><a href="#分析代码" class="headerlink" title="分析代码"></a>分析代码</h2><blockquote>
<p>def parse(self, response):</p>
</blockquote>
<hr>
<p>这个 parse() 方法通常解析response，将抓取的数据提取为dict，并查找新的URL以跟踪和创建新的请求  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">&#x27;author&#x27;</span>: quote.xpath(<span class="string">&#x27;span/small/text()&#x27;</span>).get(),</span><br><span class="line">            <span class="string">&#x27;text&#x27;</span>: quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get(),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    next_page = response.css(<span class="string">&#x27;li.next a::attr(&quot;href&quot;)&#x27;</span>).get()</span><br><span class="line">    <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">yield</span> response.follow(next_page, self.parse)</span><br><span class="line">```  </span><br><span class="line">使用CSS选择器循环 div.quote 元素，然后使用css选择器或者xpath生成一个包含 quote的作者、文本的列表  </span><br><span class="line">css选择器的使用无缝对接css中我们对选择器的认知</span><br><span class="line">xpath是官方推荐我们使用的</span><br><span class="line">查找到下一页的链接，并使用它继续调度 parse() 方法</span><br><span class="line"><span class="comment">## scrapy spider  </span></span><br><span class="line">&gt;scrapy startproject xxx</span><br><span class="line">---</span><br><span class="line">实现scrapy spider</span><br><span class="line"><span class="comment">### 第一步 创建一个新的scrapy项目  </span></span><br></pre></td></tr></table></figure>
<p>scrapy startproject xxx</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">### 第二步 定义继承spider的类并定义初始请求  </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class QuotesSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;quotes&quot;</span><br><span class="line">    start_urls = [</span><br><span class="line">        &#x27;http://quotes.toscrape.com/page/1/&#x27;,</span><br><span class="line">        &#x27;http://quotes.toscrape.com/page/2/&#x27;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        page = response.url.split(&quot;/&quot;)[-2] #根据上面的链接提取分页,如：/page/1/，提取到的就是：1</span><br><span class="line">        filename = &#x27;quotes-%s.html&#x27; % page #拼接文件名，如果是第一页，最终文件名便是：newpage-1.html</span><br><span class="line">        with open(filename, &#x27;wb&#x27;) as f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        self.log(&#x27;Saved file %s&#x27; % filename)</span><br></pre></td></tr></table></figure>
<p>还要定义属性和方法</p>
<p>name ：标识Spider。它在一个项目中必须是唯一的，也就是说，不能为不同的Spider设置相同的名称。</p>
<p>start_urls：具有URL列表的类属性</p>
<p>parse() ：将被调用来处理这些URL的每个请求。parse() 是Scrapy的默认回调方法，对没有显式分配回调的请求调用该方法。</p>
<p>parse() 方法通常解析响应，将抓取的数据提取为dict，并查找新的URL以跟踪和创建新的请求。  </p>
<p><em><strong>scrapy crawl quotes</strong></em><br>妈的，学会了，不想写了<del>_</del>。  </p>
<h3 id="scarpy-shell"><a href="#scarpy-shell" class="headerlink" title="scarpy shell"></a>scarpy shell</h3><p>终端输入：</p>
<blockquote>
<p>scrapy shell url  </p>
</blockquote>
<p>response.css()<br>response.css(‘xxx’) 返回的是一个类似列表的对象：SelectorList，<br>data中存储的是提取到的标签，<br>xpath中存储的XPath表达式，实际上CSS选择器是在后台转换为XPath表达式的<br>允许你进一步的细化选择和提取数据<br>response.css(‘xxx::text’) 与上面方法不同之处在于，data中存储的是提取到的标签的文本<br>response.css(‘xxx’).getall()、response.css(‘xxx::text’).getall()<br>getall() 方法，返回的是一个列表，<br>列表中的值是 SelectorList 中的 data 值<br>一般而言，选择器返回的结果不止一个，getall() 方法 提取全部内容  </p>
<p>response.xpath这个不写了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>quote = response.css(<span class="string">&quot;div.quote&quot;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text = quote.css(<span class="string">&quot;span.text::text&quot;</span>).get()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>text</span><br><span class="line"><span class="string">&#x27;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#x27;</span> </span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>author = quote.css(<span class="string">&quot;small.author::text&quot;</span>).get()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>author</span><br><span class="line"><span class="string">&#x27;Albert Einstein&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags = quote.css(<span class="string">&quot;div.tags a.tag::text&quot;</span>).getall()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tags</span><br><span class="line">[<span class="string">&#x27;change&#x27;</span>, <span class="string">&#x27;deep-thoughts&#x27;</span>, <span class="string">&#x27;thinking&#x27;</span>, <span class="string">&#x27;world&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="存取json文件"><a href="#存取json文件" class="headerlink" title="存取json文件"></a>存取json文件</h3><p>在项目顶层目录下输入scrapy crawl quotes -o quotes.json</p>
<p>Scrapy 会生成quotes.json文件，并将爬取到的数据放到quotes.json文件中</p>
<h3 id="具体栗子"><a href="#具体栗子" class="headerlink" title="具体栗子"></a>具体栗子</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuthorSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;author&#x27;</span></span><br><span class="line"></span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        author_page_links = response.css(<span class="string">&#x27;.author + a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(author_page_links, self.parse_author)</span><br><span class="line"></span><br><span class="line">        pagination_links = response.css(<span class="string">&#x27;li.next a&#x27;</span>)</span><br><span class="line">        <span class="keyword">yield</span> <span class="keyword">from</span> response.follow_all(pagination_links, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_author</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">extract_with_css</span>(<span class="params">query</span>):</span><br><span class="line">            <span class="keyword">return</span> response.css(query).get(default=<span class="string">&#x27;&#x27;</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">&#x27;name&#x27;</span>: extract_with_css(<span class="string">&#x27;h3.author-title::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;birthdate&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-born-date::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;birthlocation&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-born-location::text&#x27;</span>),</span><br><span class="line">            <span class="string">&#x27;bio&#x27;</span>: extract_with_css(<span class="string">&#x27;.author-description::text&#x27;</span>),</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<p>就写这么多吧。<br>本文参考CSDN博主索儿呀，索儿老师整理十分仔细，我写这个是为了记录学习爬虫的相关经历，如有冒犯，我会苏珊&gt;__&lt;<br><em><strong><a href="https://zhangguohao.blog.csdn.net/article/details/105245153">https://zhangguohao.blog.csdn.net/article/details/105245153</a></strong></em></p>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>爬虫常用库方法总结</title>
    <url>/2022/07/23/%E7%88%AC%E8%99%AB%E5%B8%B8%E7%94%A8%E5%BA%93%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<p>网页图片批量获取：</p>
<h2 id="图片下载："><a href="#图片下载：" class="headerlink" title="图片下载："></a>图片下载：</h2><p><em><strong>urllib库</strong></em>  </p>
<blockquote>
<p>urllib.request-打开和读取url<br>urllib.error-用来抛出异常<br>urllib.parse-解析url<br>urllib.robotpaser-解析robots.txt<br><img src="https://www.runoob.com/wp-content/uploads/2021/04/ulrib-py3.svg" alt="urllib">  </p>
</blockquote>
<p>urllib.request 定义了一些打开 URL 的函数和类，包含授权验证、重定向、浏览器 cookies等。</p>
<p>urllib.request 可以模拟浏览器的一个请求发起过程。</p>
<p>我们可以使用 urllib.request 的 urlopen 方法来打开一个 URL，语法格式如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)</span><br></pre></td></tr></table></figure>
<ol>
<li>url：url 地址。</li>
<li>data：发送到服务器的其他数据对象，默认为 No ne。</li>
<li>timeout：设置访问超时时间。</li>
<li>cafile 和 capath：cafile 为 CA 证书， 5.capath 为 CA 证书的路径，使用 HTTPS 需要用到。</li>
<li>cadefault：已经被弃用。</li>
<li>context：ssl.SSLContext类型，用来指定 SSL 设置。  <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line"></span><br><span class="line">myURL = urlopen(&quot;https://www.runoob.com/&quot;)</span><br><span class="line">print(myURL.read())</span><br></pre></td></tr></table></figure></li>
<li>read() 是读取整个网页内容，我们可以指定读取的长度：</li>
<li>readline() - 读取文件的一行内容</li>
<li>readlines() - 读取文件的全部内容，它会把读取的内容赋值给一个列表变量。  <blockquote>
<p>‘r’：只读。该文件必须已存在。<br>‘r+’：可读可写。该文件必须已存在，写为追加在文件内容末尾。<br>‘rb’：表示以二进制方式读取文件。该文件必须已存在。<br>‘w’：只写。打开即默认创建一个新文件，如果文件已存在，则覆盖写(即文件内原始数据会被新写入的数据清空覆盖)。<br>‘w+’：写读。打开创建新文件并写入数据，如果文件已存在，则覆盖写。<br>‘wb’：表示以二进制写方式打开，只能写文件， 如果文件不存在，创建该文件；如果文件已存在，则覆盖写。<br>‘a’：追加写。若打开的是已有文件则直接对已有文件操作，若打开文件不存在则创建新文件，只能执行写(追加在后面)，不能读。  </p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">exists = os.path.exists(&quot;foo.txt&quot;)</span><br><span class="line">if exists:</span><br><span class="line">    abspath = os.path.abspath(&quot;foo.txt&quot;)</span><br><span class="line">    print(abspath)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;文件不存在&quot;)</span><br></pre></td></tr></table></figure>
<p>**模拟头部信息<br>我们抓取网页一般需要对 headers（网页头信息）进行模拟，这时候需要使用到 urllib.request.Request 类：</p>
</li>
</ol>
<p>class urllib.request.Request(url, data&#x3D;None, headers&#x3D;{}, origin_req_host&#x3D;None, unverifiable&#x3D;False, method&#x3D;None)<br>url：url 地址。<br>data：发送到服务器的其他数据对象，默认为 None。<br>headers：HTTP 请求的头部信息，字典格式。<br>origin_req_host：请求的主机地址，IP 或域名。<br>unverifiable：很少用整个参数，用于设置网页是否需要验证，默认是False。。<br>method：请求方法， 如 GET、POST、DELETE、PUT等。  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.parse</span><br><span class="line"></span><br><span class="line">url = &#x27;https://www.runoob.com/?s=&#x27;  # 菜鸟教程搜索页面</span><br><span class="line">keyword = &#x27;Python 教程&#x27;</span><br><span class="line">key_code = urllib.request.quote(keyword)  # 对请求进行编码</span><br><span class="line">url_all = url+key_code</span><br><span class="line">header = &#123;</span><br><span class="line">    &#x27;User-Agent&#x27;:&#x27;Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36&#x27;</span><br><span class="line">&#125;   #头部信息</span><br><span class="line">request = urllib.request.Request(url_all,headers=header)</span><br><span class="line">reponse = urllib.request.urlopen(request).read()</span><br><span class="line"></span><br><span class="line">fh = open(&quot;./urllib_test_runoob_search.html&quot;,&quot;wb&quot;)    # 将文件写入到当前目录中</span><br><span class="line">fh.write(reponse)</span><br><span class="line">fh.close()</span><br></pre></td></tr></table></figure>
<p><img src="https://www.runoob.com/wp-content/uploads/2021/04/6BD0D456-E929-4C11-9118-F09C85AEA427.jpg" alt="文件内容"></p>
<p><strong>urllib.error</strong><br>urllib.error 模块为 urllib.request 所引发的异常定义了异常类，基础异常类是 URLError。</p>
<p>urllib.error 包含了两个方法，URLError 和 HTTPError。</p>
<ol>
<li><p>URLError 是 OSError 的一个子类，用于处理程序在遇到问题时会引发此异常（或其派生的异常），包含的属性 reason 为引发异常的原因。</p>
</li>
<li><p>HTTPError 是 URLError 的一个子类，用于处理特殊 HTTP 错误例如作为认证请求的时候，包含的属性 code 为 HTTP 的状态码， reason 为引发异常的原因，headers 为导致 HTTPError 的特定 HTTP 请求的 HTTP 响应头。</p>
</li>
</ol>
<p>对不存在的网页抓取并处理异常:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.error</span><br><span class="line"></span><br><span class="line">myURL1 = urllib.request.urlopen(&quot;https://www.runoob.com/&quot;)</span><br><span class="line">print(myURL1.getcode())   # 200</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    myURL2 = urllib.request.urlopen(&quot;https://www.runoob.com/no.html&quot;)</span><br><span class="line">except urllib.error.HTTPError as e:</span><br><span class="line">    if e.code == 404:</span><br><span class="line">        print(404)   # 404</span><br></pre></td></tr></table></figure>
<p><strong>urllib.parse</strong>  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=&#x27;&#x27;, allow_fragments=True)</span><br><span class="line"></span><br><span class="line">from urllib.parse import urlparse</span><br><span class="line"></span><br><span class="line">o = urlparse(&quot;https://www.runoob.com/?s=python+%E6%95%99%E7%A8%8B&quot;)</span><br><span class="line">print(o)</span><br></pre></td></tr></table></figure>
<p>*<strong><strong><strong><strong><strong>演示结果</strong></strong></strong></strong></strong>  </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ParseResult(scheme=&#x27;https&#x27;, netloc=&#x27;www.runoob.com&#x27;, path=&#x27;/&#x27;, params=&#x27;&#x27;, query=&#x27;s=python+%E6%95%99%E7%A8%8B&#x27;, fragment=&#x27;&#x27;)  </span><br></pre></td></tr></table></figure>

<p>从结果可以看出，内容是一个元组，包含 6 个字符串：协议，位置，路径，参数，查询，判断。</p>
<p>好家伙，索儿写的果真是抛砖引玉啊！  </p>
<h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>作用：正则可以使用限定符，匹配字符出现的次数，这样一来灵活度都高了   </p>
<p><img src="https://img-blog.csdnimg.cn/20200506162325642.png" alt="重复限定符">  </p>
<p><strong>正则表达式的特殊字符串</strong><br><img src="https://img-blog.csdnimg.cn/20200506162325642.png" alt="特殊字符串"></p>
<p>我终于爬出来了！哈哈哈哈。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ua_ck</span>():</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    网站需要登录才能采集，需要从Network--Doc里复制User-Agent和Cookie，Cookie要转化为字典，否则会采集失败！！！！！</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    user_agent = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">    cookies = <span class="string">&#x27;ll=&quot;118202&quot;; bid=FsStcocWuPQ; _vwo_uuid_v2=D65179C81F8EE8041E5F8605041534542|e1ed6add019a5cf6cdb06398640e7fe6; ct=y; gr_user_id=43e3a769-ff1c-4abe-b1c3-f7d5b28082de; douban-fav-remind=1; viewed=&quot;20438158_10799082_3043970_35174681_26929955_3932365_26886337_27667378_33419041_33385402&quot;; push_doumail_num=0; push_noty_num=0; __utmc=30149280; __utmc=223695111; _pk_ref.100001.4cf6=%5B%22%22%2C%22%22%2C1599712590%2C%22https%3A%2F%2Fwww.baidu.com%2Flink%3Furl%3D-946ZDrFNbdKZE0IGp73NUS3eCUaoTpabx75ZzZjM59T_FqZIgo-aeRfe2xfnu1o%26wd%3D%26eqid%3Da1f8e3670000ef4d000000065f59ad4b%22%5D; _pk_ses.100001.4cf6=*; __utma=30149280.1986063068.1597310055.1599704141.1599712590.74; __utmz=30149280.1599712590.74.69.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; __utmb=223695111.0.10.1599712590; __utma=223695111.1305332624.1597310055.1599704141.1599712590.43; __utmz=223695111.1599712590.43.40.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; ap_v=0,6.0; douban-profile-remind=1; __utmv=30149280.17799; __utmb=30149280.8.10.1599712590; dbcl2=&quot;223162585:V5BfOpq2kcs&quot;; ck=qnBB; _pk_id.100001.4cf6=991c66698d6e616d.1597310055.43.1599714528.1599704141.&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cookie转化为字典</span></span><br><span class="line">    cookies = cookies.split(<span class="string">&#x27;; &#x27;</span>)</span><br><span class="line">    cookies_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> cookies:</span><br><span class="line">        cookies_dict[i.split(<span class="string">&#x27;=&#x27;</span>)[<span class="number">0</span>]] = i.split(<span class="string">&#x27;=&#x27;</span>)[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> user_agent, cookies_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_urls</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    n:页面数量，总共有25个页面</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    urls = []</span><br><span class="line">    num = (n-<span class="number">1</span>)*<span class="number">25</span>+<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num, <span class="number">25</span>):</span><br><span class="line">        url = <span class="string">&#x27;https://movie.douban.com/top250?start=&#123;&#125;&amp;filter=&#x27;</span>.<span class="built_in">format</span>(i)</span><br><span class="line">        urls.append(url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> urls</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_movies_url</span>(<span class="params">url, u_a, c_d</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    url：每一个页面的链接</span></span><br><span class="line"><span class="string">    u_a：User-Agent</span></span><br><span class="line"><span class="string">    c_d：cookies</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    html = requests.get(url,</span><br><span class="line">                        headers=u_a,  <span class="comment"># 加载User-Agent</span></span><br><span class="line">                        cookies=c_d)  <span class="comment"># 加载cookie</span></span><br><span class="line"></span><br><span class="line">    html.encoding = html.apparent_encoding  <span class="comment"># 解决乱码的万金油方法</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> html.status_code == <span class="number">200</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;网页访问成功，代码：&#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(html.status_code))</span><br><span class="line"></span><br><span class="line">    soup = BeautifulSoup(html.text, <span class="string">&#x27;html.parser&#x27;</span>)  <span class="comment"># 用 html.parser 来解析网页</span></span><br><span class="line">    items = soup.find(<span class="string">&#x27;ol&#x27;</span>, class_=<span class="string">&#x27;grid_view&#x27;</span>).find_all(<span class="string">&#x27;li&#x27;</span>)</span><br><span class="line">    movies_url = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">        <span class="comment"># 电影链接</span></span><br><span class="line">        movie_href = item.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;hd&#x27;</span>).find(<span class="string">&#x27;a&#x27;</span>)[<span class="string">&#x27;href&#x27;</span>]</span><br><span class="line">        movies_url.append(movie_href)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> movies_url</span><br><span class="line">    time.sleep(<span class="number">0.4</span>)    <span class="comment"># 设置时间间隔，0.4秒采集一次，避免频繁登录网页 </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_movie_info</span>(<span class="params">href, u_a, c_d</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    href：每一部电影的链接</span></span><br><span class="line"><span class="string">    u_a：User-Agent</span></span><br><span class="line"><span class="string">    c_d：cookies</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    html = requests.get(href,</span><br><span class="line">                        headers=u_a,</span><br><span class="line">                        cookies=c_d)</span><br><span class="line">    soup = BeautifulSoup(html.text, <span class="string">&#x27;html.parser&#x27;</span>)  <span class="comment"># 用 html.parser 来解析网页</span></span><br><span class="line">    item = soup.find(<span class="string">&#x27;div&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;content&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    movie = &#123;&#125;  <span class="comment"># 新建字典，存放电影信息</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 电影名称</span></span><br><span class="line">    movie[<span class="string">&#x27;电影名称&#x27;</span>] = item.h1.span.text</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 导演、类型、制片国家/地区、语言、上映时间、片长（部分电影这些信息不全，先全部采集，留待数据分析时处理）</span></span><br><span class="line">    movie[<span class="string">&#x27;电影其他信息&#x27;</span>] = item.find(</span><br><span class="line">        <span class="string">&#x27;div&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;info&#x27;</span>).text.replace(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>).split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> movie[<span class="string">&#x27;电影其他信息&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;:&#x27;</span> <span class="keyword">in</span> i:</span><br><span class="line">            movie[i.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">0</span>]] = i.split(<span class="string">&#x27;:&#x27;</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 豆瓣评分、评分人数</span></span><br><span class="line">    movie[<span class="string">&#x27;评分&#x27;</span>] = item.find(<span class="string">&#x27;div&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;interest_sectl&#x27;</span>).find(</span><br><span class="line">        <span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;rating_self clearfix&#x27;</span>).find(<span class="string">&#x27;strong&#x27;</span>, class_=<span class="string">&#x27;ll rating_num&#x27;</span>).text</span><br><span class="line">    movie[<span class="string">&#x27;评分人数&#x27;</span>] = item.find(<span class="string">&#x27;div&#x27;</span>, <span class="built_in">id</span>=<span class="string">&#x27;interest_sectl&#x27;</span>).find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;rating_self clearfix&#x27;</span>).find(</span><br><span class="line">        <span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;rating_sum&#x27;</span>).find(<span class="string">&#x27;span&#x27;</span>, <span class="built_in">property</span>=<span class="string">&#x27;v:votes&#x27;</span>).text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> movie</span><br><span class="line">    time.sleep(<span class="number">0.4</span>)  <span class="comment"># 0.4秒采集一次，避免频繁登录网页</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    n:页面数量，总共有10个页面</span></span><br><span class="line"><span class="string">    u_a：User-Agent</span></span><br><span class="line"><span class="string">    c_d：cookies</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;开始采集数据，预计耗时2分钟&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理User-Agent和Cookie</span></span><br><span class="line">    login = ua_ck()</span><br><span class="line">    u_a = login[<span class="number">0</span>]</span><br><span class="line">    c_d = login[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取豆瓣top250每一页的链接，共10页</span></span><br><span class="line">    urls = get_urls(n)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;豆瓣10个网页链接已生成！！&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取每一页25部电影的链接，共250部</span></span><br><span class="line">    top250_urls = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">        result = get_movies_url(url, u_a, c_d)</span><br><span class="line">        top250_urls.extend(result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;250部电影链接采集完成！！开始采集每部电影的详细信息(预计耗时5分钟).......&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取每一部电影的详细信息</span></span><br><span class="line">    top250_movie = []  <span class="comment"># 储存每部电影的信息</span></span><br><span class="line">    error_href = []  <span class="comment"># 储存采集错误的网址</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> href <span class="keyword">in</span> top250_urls:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            movie = get_movie_info(href, u_a, c_d)</span><br><span class="line">            top250_movie.append(movie)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            error_href.append(href)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;采集失败，失败网址是&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(href))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;电影详细信息采集完成！！总共采集&#123;&#125;条数据&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(top250_movie)))</span><br><span class="line">    <span class="keyword">return</span> top250_movie, error_href</span><br><span class="line">result = main(<span class="number">10</span>)</span><br><span class="line">df = pd.DataFrame(result[<span class="number">0</span>])</span><br><span class="line">df.to_excel(<span class="string">&#x27;output1.xlsx&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>加油！！！<br><strong>书接上文</strong><br><em><strong>正则表达式</strong></em>  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment">#：匹配出，⼀个字符串第⼀个字⺟为⼤写字符，后⾯都是⼩写字⺟并且这些⼩写字⺟可有可⽆</span></span><br><span class="line">ret = re.match(<span class="string">&quot;[A-Z][a-z]*&quot;</span>,<span class="string">&quot;M&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line">ret = re.match(<span class="string">&quot;[A-Z][a-z]*&quot;</span>,<span class="string">&quot;MnnM&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line">ret = re.match(<span class="string">&quot;[A-Z][a-z]*&quot;</span>,<span class="string">&quot;Aabcdef&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line"><span class="comment">#匹配出，变量名是否有效</span></span><br><span class="line">names = [<span class="string">&quot;name1&quot;</span>, <span class="string">&quot;_name&quot;</span>, <span class="string">&quot;2_name&quot;</span>, <span class="string">&quot;__name__&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">    ret = re.match(<span class="string">&quot;[a-zA-Z_]+[\w]*&quot;</span>,name)</span><br><span class="line">    <span class="keyword">if</span> ret:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;变量名 %s 符合要求&quot;</span> % ret.group())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;变量名 %s ⾮法&quot;</span> % name)</span><br><span class="line"><span class="comment">#匹配出，0到99之间的数字</span></span><br><span class="line">ret = re.match(<span class="string">&quot;[1-9]?[0-9]&quot;</span>,<span class="string">&quot;7&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line">ret = re.match(<span class="string">&quot;[1-9]?\d&quot;</span>,<span class="string">&quot;33&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line"><span class="comment"># 这个结果并不是想要的，利⽤$才能解决</span></span><br><span class="line">ret = re.match(<span class="string">&quot;[1-9]?\d&quot;</span>,<span class="string">&quot;09&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line">ret = re.match(<span class="string">&quot;[a-zA-Z0-9_]&#123;6&#125;&quot;</span>,<span class="string">&quot;12a3g45678&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line"><span class="comment">#匹配出，8到20位的密码，可以是⼤⼩写英⽂字⺟、数字、下划线</span></span><br><span class="line">ret = re.match(<span class="string">&quot;[a-zA-Z0-9_]&#123;8,20&#125;&quot;</span>,<span class="string">&quot;1ad12f23s34455ff66&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line">```  </span><br><span class="line">**解析正则表达式**  </span><br><span class="line">&gt;[a-zA-Z_]+[\w]*  </span><br><span class="line">第一个字符是a-z或A-Z,-等字符，他们可以存在<span class="number">1</span>次或者<span class="number">00</span>次 /w匹配单词字符，即a-z、A-Z、<span class="number">0</span>-<span class="number">9</span>、_</span><br><span class="line">  </span><br><span class="line">结果：</span><br><span class="line"><span class="number">1.</span> M</span><br><span class="line"><span class="number">2.</span> Mnn</span><br><span class="line"><span class="number">3.</span> Aabcdef</span><br><span class="line"><span class="number">4.</span> 变量名 name1 符合要求</span><br><span class="line"><span class="number">5.</span> 变量名 _name 符合要求</span><br><span class="line"><span class="number">6.</span> 变量名 2_name ⾮法</span><br><span class="line"><span class="number">7.</span> 变量名 __name__ 符合要求</span><br><span class="line"><span class="number">8.</span> <span class="number">7</span></span><br><span class="line"><span class="number">9.</span> <span class="number">33</span></span><br><span class="line"><span class="number">10.</span> <span class="number">0</span></span><br><span class="line"><span class="number">11.</span> 12a3g4</span><br><span class="line"><span class="number">12.</span> 1ad12f23s34455ff66    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">匹配开头结尾  </span><br><span class="line">^ 匹配字符串开头  </span><br><span class="line">$ 匹配字符串结尾  </span><br><span class="line">```python  </span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">email_list = [<span class="string">&quot;xiaoWang@163.com&quot;</span>, <span class="string">&quot;xiaoWang@163.comheihei&quot;</span>, <span class="string">&quot;.com.xiaowang@qq.com&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> email <span class="keyword">in</span> email_list:</span><br><span class="line">    ret = re.match(<span class="string">&quot;[\w]&#123;4,20&#125;@163\.com$&quot;</span>, email)</span><br><span class="line">    <span class="keyword">if</span> ret:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 是符合规定的邮件地址,匹配后的结果是:%s&quot;</span> % (email, ret.group()))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 不符合要求&quot;</span> % email)  </span><br><span class="line">```  </span><br><span class="line"> </span><br><span class="line">xiaoWang@<span class="number">163.</span>com 是符合规定的邮件地址,匹配后的结果是:xiaoWang@<span class="number">163.</span>com</span><br><span class="line">xiaoWang@<span class="number">163.</span>comheihei 不符合要求</span><br><span class="line">.com.xiaowang@qq.com 不符合要求  </span><br><span class="line">**匹配分组**  </span><br><span class="line"> |	匹配左右任意⼀个表达式  </span><br><span class="line">(ab)	将括号中字符作为⼀个分组  </span><br><span class="line">\num	引⽤分组num匹配到的字符串  </span><br><span class="line">(?P&lt;name&gt;)	分组起别名，匹配到的子串组在外部是通过定义的 name 来获取的  </span><br><span class="line">(?P=name)	引⽤别名为name分组匹配到的字符串 </span><br><span class="line">---  ---</span><br><span class="line">举个例子：|  </span><br><span class="line">```python</span><br><span class="line"> <span class="comment">#匹配出0-100之间的数字</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">ret = re.match(<span class="string">&quot;[1-9]?\d$|100&quot;</span>,<span class="string">&quot;8&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group()) <span class="comment"># 8</span></span><br><span class="line">ret = re.match(<span class="string">&quot;[1-9]?\d$|100&quot;</span>,<span class="string">&quot;78&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group()) <span class="comment"># 78</span></span><br><span class="line">ret = re.match(<span class="string">&quot;[1-9]?\d$|100&quot;</span>,<span class="string">&quot;08&quot;</span>)</span><br><span class="line"><span class="comment"># print(ret.group()) # 不是0-100之间</span></span><br><span class="line">ret = re.match(<span class="string">&quot;[1-9]?\d$|100&quot;</span>,<span class="string">&quot;100&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group()) <span class="comment"># 100</span></span><br></pre></td></tr></table></figure>
<hr>
<blockquote>
<p>“[1-9]?\d$|100”匹配的是0~99和100   </p>
</blockquote>
<p>举个例子：（）  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#需求：匹配出163、126、qq邮箱</span></span><br><span class="line">ret = re.match(<span class="string">&quot;\w&#123;4,20&#125;@163\.com&quot;</span>, <span class="string">&quot;test@163.com&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group()) <span class="comment"># test@163.com</span></span><br><span class="line">ret = re.match(<span class="string">&quot;\w&#123;4,20&#125;@(163|126|qq)\.com&quot;</span>, <span class="string">&quot;test@126.com&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group()) <span class="comment"># test@126.com</span></span><br><span class="line">ret = re.match(<span class="string">&quot;\w&#123;4,20&#125;@(163|126|qq)\.com&quot;</span>, <span class="string">&quot;test@qq.com&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group()) <span class="comment"># test@qq.com</span></span><br><span class="line">ret = re.match(<span class="string">&quot;\w&#123;4,20&#125;@(163|126|qq)\.com&quot;</span>, <span class="string">&quot;test@gmail.com&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> ret:</span><br><span class="line">    <span class="built_in">print</span>(ret.group())</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;不是163、126、qq邮箱&quot;</span>) <span class="comment"># 不是163、126、qq邮箱</span></span><br><span class="line"><span class="comment">#不是以4、7结尾的⼿机号码(11位)</span></span><br><span class="line">tels = [<span class="string">&quot;13100001234&quot;</span>, <span class="string">&quot;18912344321&quot;</span>, <span class="string">&quot;10086&quot;</span>, <span class="string">&quot;18800007777&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> tel <span class="keyword">in</span> tels:</span><br><span class="line">    ret = re.match(<span class="string">&quot;1\d&#123;9&#125;[0-35-68-9]&quot;</span>, tel)</span><br><span class="line">    <span class="keyword">if</span> ret:</span><br><span class="line">        <span class="built_in">print</span>(ret.group())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 不是想要的⼿机号&quot;</span> % tel)</span><br><span class="line"><span class="comment">#提取区号和电话号码</span></span><br><span class="line">ret = re.match(<span class="string">&quot;([^-]*)-(\d+)&quot;</span>,<span class="string">&quot;010-12345678&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(ret.group())</span><br><span class="line"><span class="built_in">print</span>(ret.group(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(ret.group(<span class="number">2</span>))      </span><br></pre></td></tr></table></figure>

<p>解析</p>
<blockquote>
<p>\w{4,20}@(163|126|qq).com<br>匹配4-20位的qq，126，136邮箱<br>举例：\number</p>
</blockquote>
<p>匹配数字代表的组合。每个括号是一个组合，组合从1开始编号。比如 (.+) \1 匹配 ‘the the’ 或者 ‘55 55’, 但不会匹配 ‘thethe’ (注意组合后面的空格)。这个特殊序列只能用于匹配前面99个组合。如果 number 的第一个数位是0， 或者 number 是三个八进制数，它将不会被看作是一个组合，而是八进制的数字值。在 ‘[‘ 和 ‘]’ 字符集合内，任何数字转义都被看作是字符。</p>
<p>例子1：匹配出 <html>hh</html><br>\1,…,\9，匹配第n个分组的内容。如例子所示，指匹配第一个分组的内容。  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 正确的理解思路：如果在第⼀对&lt;&gt;中是什么，按理说在后⾯的那对&lt;&gt;中就应该是什么。通过引⽤分组中匹配到的数据即可，但是要注意是元字符串，即类似 r&quot;&quot;这种格式。</span></span><br><span class="line">ret = re.match(<span class="string">r&quot;&lt;([a-zA-Z]*)&gt;\w*&lt;/\1&gt;&quot;</span>, <span class="string">&quot;&lt;html&gt;hh&lt;/html&gt;&quot;</span>)</span><br><span class="line"><span class="comment"># 因为2对&lt;&gt;中的数据不⼀致，所以没有匹配出来</span></span><br><span class="line">test_label = [<span class="string">&quot;&lt;html&gt;hh&lt;/html&gt;&quot;</span>,<span class="string">&quot;&lt;html&gt;hh&lt;/htmlbalabala&gt;&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> test_label:</span><br><span class="line">    ret = re.match(<span class="string">r&quot;&lt;([a-zA-Z]*)&gt;\w*&lt;/\1&gt;&quot;</span>, label)</span><br><span class="line">    <span class="keyword">if</span> ret:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 这是一对正确的标签&quot;</span> % ret.group())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 这是⼀对不正确的标签&quot;</span> % label)  </span><br></pre></td></tr></table></figure>

<hr>
<p>举例：(?P<name>) (?P&#x3D;name)</p>
<p>一个用于标记，一个用于在同一个正则表达式中复用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">labels = [<span class="string">&quot;&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h1&gt;&lt;/html&gt;&quot;</span>, <span class="string">&quot;&lt;html&gt;&lt;h1&gt;www.itcast.cn&lt;/h2&gt;&lt;/html&gt;&quot;</span>]</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">    ret = re.match(<span class="string">r&quot;&lt;(\w*)&gt;&lt;(\w*)&gt;.*&lt;/\2&gt;&lt;/\1&gt;&quot;</span>, label)</span><br><span class="line">    <span class="keyword">if</span> ret:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 是符合要求的标签&quot;</span> % ret.group())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s 不符合要求&quot;</span> % label) </span><br><span class="line">```  </span><br><span class="line">---  </span><br><span class="line">re.<span class="built_in">compile</span> 函数</span><br><span class="line"><span class="built_in">compile</span> 函数用于编译正则表达式，生成一个正则表达式（ Pattern ）对象，供 match() 和 search() 这两个函数使用。</span><br><span class="line">``` python </span><br><span class="line">prog = re.<span class="built_in">compile</span>(pattern)</span><br><span class="line">result = prog.match(string)  </span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">等价于</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">result = re.match(pattern, string)   </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\d+&#x27;</span>)   </span><br><span class="line">m = pattern.match(<span class="string">&#x27;one12twothree34four&#x27;</span>, <span class="number">3</span>, <span class="number">10</span>) <span class="comment"># 从&#x27;1&#x27;的位置开始匹配，正好匹配</span></span><br><span class="line"> <span class="built_in">print</span> m                                         <span class="comment"># 返回一个 Match 对象</span></span><br><span class="line">&lt;_sre.SRE_Match <span class="built_in">object</span> at <span class="number">0x10a42aac0</span>&gt;</span><br><span class="line">m.group(<span class="number">0</span>)   <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="string">&#x27;12&#x27;</span></span><br><span class="line">m.start(<span class="number">0</span>)   <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"> m.end(<span class="number">0</span>)     <span class="comment"># 可省略 0</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line">m.span(<span class="number">0</span>)    <span class="comment"># 可省略 0</span></span><br><span class="line">(<span class="number">3</span>, <span class="number">5</span>)  </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在上面，当匹配成功时返回一个 Match 对象，其中：</p>
<p>group([group1, …]) 方法用于获得一个或多个分组匹配的字符串，当要获得整个匹配的子串时，可直接使用 group() 或 group(0)；<br>start([group]) 方法用于获取分组匹配的子串在整个字符串中的起始位置（子串第一个字符的索引），参数默认值为 0；<br>end([group]) 方法用于获取分组匹配的子串在整个字符串中的结束位置（子串最后一个字符的索引+1），参数默认值为 0；<br>span([group]) 方法返回 (start(group), end(group))</p>
]]></content>
      <tags>
        <tag>python_爬虫</tag>
      </tags>
  </entry>
</search>
